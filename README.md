# Case Data Master Eng Dados

---

# Solu√ß√£o proposta

Pipeline Streaming para consumo e analise de dados do twitter.

![Untitled](img_readme/Untitled.png)

# Desenho T√©cnico

## Diagrama da solu√ß√£o

![Untitled](img_readme/Untitled%201.png)

*Construi o case usando containers docker em cada componente , para facilitar a implanta√ß√£o do case independente do ambiente.*

## Tecnologias Usadas

---

### 1 - API do Twitter + Servi√ßo Python

### API do Twitter

Para o consumo dos dados do twitter foi usada a API V2 disponibilizada pelo pr√≥prio twitter , com o seguinte endpoint: `https://api.twitter.com/2/tweets/sample/stream`, com ela temos acesso a 1% de todos os twitter processados naquele momento.

### Servi√ßo Python

Para consumir esse stream de dados e gravar no Kafka constru√≠ um servi√ßo em python utilizando a lib ***requests*** para fazer as requisi√ß√µes.

Para exemplificar a arquitetura selecionei apenas algumas informa√ß√£o dos tweets, segue um exemplo:

```json
{"data":{"id":"1561891520416022529","lang":"pt","text":"RT @rmotta2: Ca√≠ em um sono desorganizado e agitado. Em um certo momento, senti o toque de um len√ßol caindo sobre mim. Algu√©m me cobre, aje‚Ä¶"}}

{"data":{"id":"1561891524576780289","lang":"pt","text":"RT @jessicabatan: Oi, gente! Estou fazendo essa vakinha porque preciso comprar meu primeiro computador. Sempre tive usados que me emprestar‚Ä¶"}}

{"data":{"id":"1561891524614422528","lang":"pt","text":"RT @sensacionalista: Bolsonaro faz hora extra e mente mais 40 minutos depois do expediente https://t.co/cxfM3BjtV0"}}

{"data":{"id":"1561891524606058496","lang":"pt","text":"@randolfeap Antes de 48 horas j√° estava chegando \n\nMais como o Makron disse que a amaz√¥nia e o pulm√£o do mundo esperava que todo planeta visse isso"}}
```

Com os tweets em m√£os , o servi√ßo os ‚Äúserializa‚Äù em bytecode e manda para o t√≥pico Kafka de nome :`twitter_topic_full_en`.

### 2- Kafka

Como forma de persistir os dados do stream do twitter, garantir a integridade do dado e performance escolhi o Kafka como middleware de mensageira, nele tenho um t√≥pico que para fins de teste est√° com o replication e o acks em 1 (onde o Kafka confirma para o producer que o dado est√° gravado no leader).

Exemplo da mensagem gravada no Kafka:

![Untitled](img_readme/Untitled%202.png)

### 3 - Spark Structured Streaming

Para processar e gerar valor sobre os dados, escolhi como ferramenta de processamento o Spark Structured Streaming, ele usa a tecnologia de micro batchs que consiste em tratar o streaming em pequenos batchs de dados e com isso utilizar as APIs SQL sobre os dados ,fornecendo uma ampla gama de analises e tratamentos.

(Utilizei a API `Python`  , a vers√£o utilizada do Spark foi a `3.2.2` com Scala na vers√£o `2.12`, para conex√£o ao Kafka utilizei o pacote `spark-sql-kafka-0-10_2` e para conectar ao Elastic o `elasticsearch-spark-30_2.12:7.12.0`)

O Spark consome as mensagens que v√£o sendo postadas no t√≥pico `twitter_topic_full_en` , para car√°ter de teste estou lendo sempre as mensagens do seu primeiro offset.

Exemplo de leitura de dados de um t√≥pico Kafka

```python
(spark.readStream
        .format("kafka")
        .option("kafka.bootstrap.servers", "kafka:29092")
        .option("subscribe", "twitter_topic_full_en")
        .option("startingOffsets", "earliest")
        .load())
```

Com as mensagens no Spark, processo o conte√∫do de cada Twitter para retirar car√°cteres especiais, urls e etc, deixando o texto preparado para passar em um modelo Machine Learning  que faz classifica√ß√£o de sentimento e com isso categorizar em 3 n√≠veis:

- Negative
- Neutral
- Positive

Ap√≥s ter classificado o tweet o Spark envia resultado do processamento para o index `sentiment_labels` do ElasticSearch atrav√©s do comando:

```python
(
	words.writeStream
  .outputMode("append")
  .format("org.elasticsearch.spark.sql")
  .option("checkpointLocation", "logs_elastic_clean")
  .start("sentiment_labels").awaitTermination()
)
```

*nesse comando temos a option **checkpointLocation**, que √© o local onde o spark vai armazenar os metadados do processamento para , caso ocorra algum problema ele consiga seguir de onde parou as inser√ß√µes.* 

### 4 - ElasticSearch

Para fornecer uma forma r√°pida e robusta de consultar os dados processados, escolhi o ElasticSearch pela sua baixa lat√™ncia para consultas anal√≠ticas em grandes volumes de dados , devido a forma de indexa√ß√£o dos dados.

Para efeito de teste, tenho um index chamado `sentiment_labels` com 1 shard  e com 1 replica, informa√ß√µes essas que podem ser acessadas pelo endpoint `/sentiment_labels/_search`.

A politica de lifecycle dos dados est√° a default , que pode ser acessada pelo endpoint `/_ilm/policy` .

Os dados ficam armazenados no elastic em forma de JSONs, um exemplo do formato armazenado no index esta na chave `_source` apresentada abaixo:

```json
{
"_index": "sentiment_labels",
"_type": "_doc",
"_id": "79L9VYIBKakR1Hew0CZc",
"_score": 1,
"_source": {
"word": "  Without makeup, Beauty and Daniella are beautiful üòç BBNajia",
"polarity": "0.85",
"subjectivity": "1.0",
"sentiment_analysis": "Positive"
}
},
{
"_index": "sentiment_labels",
"_type": "_doc",
"_id": "8NL9VYIBKakR1Hew0CZc",
"_score": 1,
"_source": {
"word": "  oh, how rain amuses itself in sorrows\nits what someone once told me...\nhowever, i saw it differently~\n\nrain was nature's‚Ä¶",
"polarity": "0.0",
"subjectivity": "0.6",
"sentiment_analysis": "Neutral"
}
},
```

O ElasticSearch ainda nos permite atrav√©s do endpoint `/_sql` fazer consultas em formato SQL em seus dados, onde ele trata cada index como uma tabela, segue exemplos:

**Query para contar a quantidade de valores no index sentiment_labels**

```bash
curl --request POST \
  --url 'http://3.81.175.61:9200/_sql?format=json' \
  --header 'Content-Type: application/json' \
  --data '{
  "query": "SELECT count(*) FROM sentiment_labels"
}'
```

**Resposta:**

```json
{
	"columns": [
		{
			"name": "count(*)",
			"type": "long"
		}
	],
	"rows": [
		[
			155496
		]
	]
}
```

*√â poss√≠vel alterar a forma como o resultado ser√° devolvido atrav√©s do query params `format=`*

**Query que agrupa a analise de sentimento e ordena por maior representatividade, aqui usando o `format=txt` vemos que sa√≠da vem com um formato diferente**

```bash
curl --request POST \
  --url 'http://3.81.175.61:9200/_sql?format=txt' \
  --header 'Content-Type: application/json' \
  --data '{
  "query": "SELECT sentiment_analysis,COUNT(*) AS qtd FROM sentiment_labels GROUP BY sentiment_analysis ORDER BY COUNT(*) DESC"
}'
```

**Resposta**

```
sentiment_analysis|      qtd      
------------------+---------------
Neutral           |82538          
Positive          |65767          
Negative          |26786
```

### 5 - Kibana

Como forma de facilitar as analises e visualiza√ß√µes dos resultados gerados pelo pipeline, utilizei o Kibana pela sua integra√ß√£o nativa com o ElasticSearch.

**No Kibana criei o um index pattern com o nome `sen*`:**

![Untitled](img_readme/Untitled%203.png)

E com base nesse index pattern foram criadas duas visualiza√ß√µes utilizando o Lens, que nos permite criar visualiza√ß√£o de uma forma intuitiva atrav√©s de drag and drop.

**Contagem dos tweets processados**

![Untitled](img_readme/Untitled%204.png)

**Agrupamento por tipo de sentimento**

![Untitled](img_readme/Untitled%205.png)

Com essas 2 visualiza√ß√µes, crie um DashBoard e nele coloquei uma atualiza√ß√£o autom√°tica de 2 segundos para acompanhar a evolu√ß√£o dos dados ingestados.

![Untitled](img_readme/Untitled%206.png)

---

# Reproduzindo o ambiente

## Pr√© Requisitos

### Docker

O projeto est√° em docker e utiliza o docker-compose como orquestrador dos containers, para conseguir reproduzir √© necess√°ria a sua instala√ß√£o.

Link para a instala√ß√£o do Docker e Docker Compose

[Install Docker Engine](https://docs.docker.com/engine/install/)

[Install Docker Compose](https://docs.docker.com/compose/install/)

### Bearer Token Twitter

Para rodar o projeto √© necess√°rio ter uma conta de desenvolvedor no Twitter, e com ela pegar seu API Bearer Token que ser√° utilizado pelo servi√ßo Python que vai repassar o streaming do Twitter para o Kafka.

Link para criar a conta do Twitter: 

[Use Cases, Tutorials, & Documentation](https://developer.twitter.com/en)

### Instalar o GIT

O c√≥digo fonte do projeto est√° hospedado no GitHub, para conseguir acessa-lo √© necess√°rio ter o GIT instalado em sua maquina.

Link para instala√ß√£o do GIT:

[Git - Installing Git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git)

## Instala√ß√£o

### Clonar o reposit√≥rio

Abra o terminal da sua escolha e digite o seguinte comando.

```bash
git clone https://github.com/sjose03/data-master.git
```

Ap√≥s clonar o reposit√≥rio, navegar at√© o diret√≥rio.

Necess√°rio criar um arquivo .env e nele colocar o seu token gerado pelo twitter no seguinte formato:

```
BEARER_TOKEN='<token do twitter>'
```

Ap√≥s criar o arquivo, executar o seguinte comando para criar os containers, networks e volumes.

```dockerdocker buildx create --use
docker-compose up -d .
```

***A op√ß√£o -d ir√° deixar o seu terminal livre para que possa usar ele para acessar os containers.***

No seu navegador voce poder√° acessar os seus containers atrav√©s dos seguintes endere√ßos:

- [localhost:19000](http://localhost:19000) - Interface gr√°fica para analisar os t√≥picos do Kafka

![Untitled](img_readme/Untitled%207.png)

- [localhost:9200/sentiment_labels/_search](http://localhost:9200/sentiment_labels/_search) - Link para abrir o ElasticSearch e mostrar os tweets processados
    
    ![Untitled](img_readme/Untitled%208.png)
    
- [localhost:5601](http://localhost:5601) - Endere√ßo para acessar o Kibana
    
    ![Untitled](img_readme/Untitled%209.png)
    

Ap√≥s tudo ligado, vamos importar a visualiza√ß√£o e o index pattern no kibana atrav√©s do endere√ßo [http://localhost:5601/app/management/kibana/objects](http://localhost:5601/app/management/kibana/objects)` , na op√ß√£o import e escolher o arquivo `export.ndjson`

![Untitled](img_readme/Untitled%2010.png)

Ap√≥s fazer os imports, teremos os seguintes objetos:

![Untitled](img_readme/Untitled%2011.png)

Podemos acessar o dashboard agora atrav√©s do endere√ßo:

`http://localhost:5601/app/dashboards#/view/3ce2bf50-1112-11ed-9c89-7b2785879126?_g=(filters:!(),refreshInterval:(pause:!f,value:2000),time:(from:now-15m,to:now))&_a=(description:'',filters:!(),fullScreenMode:!f,options:(hidePanelTitles:!f,useMargins:!t),query:(language:kuery,query:''),tags:!(),timeRestore:!f,title:'analise%20twets',viewMode:view)`
